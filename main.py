"""READMEThis script extracts data from GLM .nc files and MMIA .cdf files and preparesit for comparison to extract detection sensitivity of GLM.To do so, a time snippet for every day with GLM and MMIA files is extractedto use only instead of the whole signal vectors using LINET's data from a.csv file, where date, time, latitude, longitude, trigger ID and group_IDare taken.Every file from a given path 'GLM_files_path' (for GLM files) and'MMIA_files_path' (for MMIA files) is classified in a different directoryaccording to the snippet (given by LINET's data) if MMIA, or date (day) ofthe event if GLM, and stored in given paths 'GLM_ordered_dir' and'MMIA_Ordered_dir' (for GLM and MMIA, respectively).                            CAUTION!This step needs to be run by a UNIX-like OS, as explicit terminal commandsare given using Python's library "os".As the files have been ordered, data extraction is done using a Pythonscript for GLM and a MatLab script for MMIA, both given by Jesús López, onlyfor the matching dates between the existing GLM and MMIA files. This mainscript initialises the MatLab engine by its own. This extracted data isstored in different snippet .txt files for GLM and .mat files for MMIA atgiven paths 'GLM_ordered_outputs' and 'MMIA_ordered_outputs'.                            IMPORTANTExtracting data from GLM's .nc files and MMIA's .cdf files takes a lot of timeand is a one-time step, specially for MMIA .cdf files where a MatLab enginehas to be started for every snippet. Because of that, one can set specialparameters 'pre_extracted_GLM' and 'pre_extracted_MMIA' to '0' to make thatextraction once, and then set those parameters to '1' to bypass the extractingoperations as the data has already been stored in '..._ordered_outputs'.Again, this process needs to be run by a UNIX-like OS.This data is then uploaded to the Python workspace and treated to allowa comparison analysis using cross-correlation.For GLM data, an integration of radiance is made every 0.002s (GLM sample rate)and time and signal vectors are accommodated to MMIA sample rate of 0.00001s.For MMIA data, afilter is applied to the 777.4nm photometer data vector inorder to reduce signal noise, and again accommodated to MMIA sample ratein case any time jump was there.Both types of data are then normalised and cross-correlated to get the timeshift between them, aligned and compared by counting peaks in their signals.All functions are imported from 'TFG_library.py' to make this script simpleand clean.@ Jaime Francisco Morán Domínguez, 2021"""import library as TFGimport numpy as npimport matplotlib.pyplot as pltimport pickle# Just for plot presentation in LaTeX Style (slows the program)#plt.rc('font', **{'family': 'serif', 'serif': ['latin modern roman']})'''#####################################################               USER INPUT DATA                 #####################################################'''### GENERAL #### Boolean variable for generating plotsshow_plots = False### GLM #### Time in seconds to analyze GLM and MMIA before and after LINET's time snippetcropping_margin = 0.5# Plus of angle in latitude and longitude to snip GLM dataGLM_radius = 400 # [km]angle_margin = GLM_radius / 111.11 # or a given value in degrees [º]# Boolean variable for downloading GLM .nc files from Google Cloud Storagepre_downloaded_GLM = True# Boolean variable for pre-extracted filespre_extracted_GLM = False# Boolean variable for integrating GLM signals if not pre-donepre_integrated_GLM = False### MMIA #### Boolean variable for pre-extracted filespre_extracted_MMIA = True# Boolean variable for conditioning MMIA data if not done beforepre_conditioned_MMIA = False# Path to Hard Disk with all MMIA files#ssd_path = '/Volumes/Jaime_F_HD/mmia_2020'ssd_path = '/Users/jaimemorandominguez/Desktop/test_descarga_GLM'# Path where MMIA's .cdf files are located#MMIA_files_path = '/Volumes/Jaime_F_HD/mmia_2020/mmia_20'MMIA_files_path = '/Users/jaimemorandominguez/Desktop/test_cdf'# Maximum length in seconds of each triggertrigger_length = 2 # [s]# Threshold for MMIA signalmmia_threshold = 1.75   # [micro W / m^2]'''#####################################################           END OF USER INPUT DATA              #####################################################'''# New directories paths needed in the futureGLM_ordered_dir = ssd_path + '/glm_downl_nc_files'GLM_ordered_outputs = ssd_path + '/glm_txt'MMIA_mats_path = ssd_path + '/mmia_mat'###### MMIA'S TRIGGER CHARACTERIZATION ######[matches, trigger_filenames] = TFG.get_MMIA_triggers(MMIA_files_path, trigger_length)###### MMIA'S DATA ORDERING, EXTRACTION, UPLOAD AND CONDITIONING ####### File ordering and data extractionif pre_extracted_MMIA == False:        TFG.create_MMIA_trigger_directories(matches, trigger_filenames, MMIA_files_path, ssd_path)    [mmia_raw, trigger_limits] = TFG.extract_trigger_info(ssd_path, trigger_filenames, matches)else:    print('All MMIA data was pre-extracted, uploading from %s \n' % MMIA_mats_path)    [mmia_raw, trigger_limits] = TFG.upload_MMIA_mats(ssd_path, trigger_filenames, matches)if pre_conditioned_MMIA == False:    # Conditioning MMIA data for further analysis    MMIA_filtered = TFG.condition_MMIA_data(mmia_raw, matches, show_plots, mmia_threshold)    print('Saving MMIA conditioned data...')    f = open('mmia_filtered_data.pckl', 'wb')    pickle.dump(MMIA_filtered, f)    f.close()    print('Done!\n')else:    print('MMIA data was pre-conditioned. Uploading from %s/mmia_filtered_data.pckl...' % ssd_path)    f = open('mmia_filtered_data.pckl', 'rb')    MMIA_filtered = pickle.load(f)    f.close()    print('Done!\n')del mmia_raw########### GLM'S DATA DOWNLOAD, EXTRACTION, UPLOAD AND CONDITIONING ############ Downloading GLM data from Google Cloud Servicesif pre_downloaded_GLM == False:        TFG.download_GLM(ssd_path, trigger_filenames, MMIA_filtered, matches)del trigger_filenames# Extracting GLM data into trigger .txt filesif pre_extracted_GLM == False:    TFG.extract_GLM(GLM_ordered_dir, GLM_ordered_outputs, trigger_limits, matches, MMIA_filtered, angle_margin, cropping_margin)del trigger_limits# Uploading and integrating GLM signalif pre_integrated_GLM == False:        # Unifying all data in a structure of matrices    GLM_raw_data = TFG.unify_GLM_data(GLM_ordered_outputs, MMIA_filtered, matches, show_plots)    # Conditioning GLM data for further analysis    GLM_data = TFG.condition_GLM_data(GLM_raw_data, matches, show_plots)    del GLM_raw_data        print('Saving GLM integrated data...')    f = open('glm_integrated_data.pckl', 'wb')    pickle.dump(GLM_data, f)    f.close()    print('Done!\n')else:    print('GLM data was pre-integrated. Uploading from %s/glm_integrated_data.pckl...' % ssd_path)    f = open('glm_integrated_data.pckl', 'rb')    GLM_data = pickle.load(f)    f.close()    print('Done!\n')########### CROSS-CORRELATION ############ Normalizing GLM data to cross-correlate with MMIA dataprint('Normalizing GLM data...')GLM_norm = [None] * len(GLM_data)for i in range(len(GLM_data)):    snip = [None] * len(GLM_data[i])    GLM_norm[i] = snipfor i in range(len(GLM_data)):    for j in range(len(GLM_data[i])):        if type(GLM_data[i][j]) == np.ndarray:            snippet = np.zeros((len(GLM_data[i][j]),2))            GLM_norm[i][j] = snippet            GLM_norm[i][j][:,0] = GLM_data[i][j][:,0]            GLM_norm[i][j][:,1] = TFG.normalize(GLM_data[i][j][:,1])print('Done!')print(' ')# Normalizing MMIA data to cross-correlate with GLM dataprint('Normalizing MMIA data...')MMIA_norm = [None] * len(MMIA_filtered)for i in range(len(MMIA_filtered)):    snip = [None] * len(MMIA_filtered[i])    MMIA_norm[i] = snipfor i in range(len(MMIA_filtered)):    for j in range(len(MMIA_filtered[i])):        if type(MMIA_filtered[i][j]) == np.ndarray:            snippet = np.zeros((len(MMIA_filtered[i][j]),2))            MMIA_norm[i][j] = snippet            MMIA_norm[i][j][:,0] = MMIA_filtered[i][j][:,0]            MMIA_norm[i][j][:,1] = TFG.normalize(MMIA_filtered[i][j][:,1])print('Done!')print(' ')# Cross-correlating snippets[GLM_xcorr, MMIA_xcorr, GLM_xcorr_norm, MMIA_xcorr_norm, delays] = TFG.cross_correlate_GLM_MMIA(GLM_data, MMIA_filtered, GLM_norm, MMIA_norm, matches, show_plots)del GLM_datadel MMIA_filtered# Saving cross-correlated dataf = open('GLM_MMIA_xcorr_data.pckl', 'wb')pickle.dump([matches, GLM_xcorr, MMIA_xcorr, GLM_xcorr_norm, MMIA_xcorr_norm], f)f.close()'''# Getting peaks from cross-correlated signals[GLM_peaks, MMIA_peaks] = TFG.get_GLM_MMIA_peaks(GLM_xcorr, MMIA_xcorr, matches, show_plots)f = open('peaks_data.pckl', 'wb')pickle.dump([GLM_peaks, MMIA_peaks], f)f.close()del GLM_normdel MMIA_norm# Getting delay statistics[total_snippets, avg_all, std_all, avg_MMIA_delay, std_MMIA_delay, avg_GLM_delay, std_GLM_delay, MMIA_delays, GLM_delays, no_delays] = TFG.study_delays(delays, GLM_xcorr, MMIA_xcorr, show_plots)'''