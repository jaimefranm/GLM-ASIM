import library_part as TFGimport numpy as npimport matplotlib.pyplot as pltimport pickleimport os# Just for plot presentation in LaTeX Style (slows the program)#plt.rc('font', **{'family': 'serif', 'serif': ['latin modern roman']})'''#####################################################               USER INPUT DATA                 #####################################################'''### GENERAL #### Boolean variable for setting everything for the first executionfirst_execution = False# Boolean variable for generating plotsshow_plots = False# Boolean variable for pre-cross-correlated datapre_xc = True# Boolean variable for pre-oredered triggers in directoriespre_trigger_directories = True# Path to Hard Disk (with all MMIA files and where to store all files)#ssd_path = '/Volumes/Jaime_F_HD/mmia_2020'ssd_path = '/Users/jaimemorandominguez/Desktop/test_descarga_GLM'#ssd_path = '/media/lrg'# Path where MMIA's .cdf files are located#MMIA_files_path = '/Volumes/Jaime_F_HD/mmia_2020/mmia_20'MMIA_files_path = '/Users/jaimemorandominguez/Desktop/test_cdf'#MMIA_files_path = '/media/lrg/mmia_20'### GLM #### Time in seconds to analyze GLM and MMIA before and after LINET's time snippetcropping_margin = 0.5# Plus of angle in latitude and longitude to snip GLM dataGLM_radius = 400 # [km]angle_margin = GLM_radius / 111.11 # or a given value in degrees [ยบ]# Boolean variable for downloading GLM .nc files from Google Cloud Storagepre_downloaded_GLM = True# Boolean variable for pre-extracted filespre_extracted_GLM = True# Boolean variable for integrating GLM signals if not pre-donepre_integrated_GLM = True### MMIA #### Boolean variable for pre-extracted filespre_extracted_MMIA = True# Boolean variable for conditioning MMIA data if not done beforepre_conditioned_MMIA = True# Maximum length in seconds of each triggertrigger_length = 2 # [s]# Threshold for MMIA signalmmia_threshold = 1.75   # [micro W / m^2]'''#####################################################           END OF USER INPUT DATA              #####################################################'''# New directories paths needed in the futuregeneral_variables_path = ssd_path + '/general_variables_bin'GLM_ordered_dir = ssd_path + '/glm_downl_nc_files'GLM_ordered_outputs = ssd_path + '/glm_txt'GLM_integrated_bin = ssd_path + '/glm_int_bin'GLM_xcorr_bin = ssd_path + '/glm_xcorr_bin'MMIA_mats_path = ssd_path + '/mmia_mat'MMIA_filtered_bin = ssd_path + '/mmia_filt_bin'MMIA_xcorr_bin = ssd_path + '/mmia_xcorr_bin'###### MMIA'S TRIGGER CHARACTERIZATION ######if first_execution == True:        pre_downloaded_GLM = False    pre_extracted_GLM = False    pre_integrated_GLM = False    pre_extracted_MMIA = False    pre_conditioned_MMIA = False    pre_xc = False    [matches, trigger_filenames] = TFG.get_MMIA_triggers(MMIA_files_path, trigger_length)        os.system('mkdir ' + general_variables_path)        # Saving 'matches' variable into a binary    f = open(general_variables_path+'/matches.pckl', 'wb')    pickle.dump(matches, f)    f.close()        else:        # Uploading matches from binary    f = open(general_variables_path+'/matches.pckl', 'rb')    matches = pickle.load(f)    f.close()if pre_trigger_directories == False:        TFG.create_MMIA_trigger_directories(matches, trigger_filenames, MMIA_files_path, ssd_path)################################################################################   From this point every step is made for every day with existing MMIA data###############################################################################for day in range(len(matches)):        current_day = matches[day]        ###### MMIA'S DATA ORDERING, EXTRACTION, UPLOAD AND CONDITIONING ######    # File ordering and data extraction    if pre_extracted_MMIA == False:        [mmia_raw, trigger_limits] = TFG.extract_trigger_info(ssd_path, trigger_filenames, matches, day)    else:        print('All MMIA data was pre-extracted, uploading from %s...' % MMIA_mats_path)        [mmia_raw, trigger_limits] = TFG.upload_MMIA_mats(ssd_path, trigger_filenames, matches, day)        print('Done!\n')    if pre_conditioned_MMIA == False:        # Conditioning MMIA data for further analysis        MMIA_filtered = TFG.condition_MMIA_data(mmia_raw, matches, show_plots, mmia_threshold, day)        print('Saving MMIA conditioned data for day %s...' % matches[day])        f = open(MMIA_filtered_bin + '/' + matches[day] + '.pckl', 'wb')        pickle.dump(MMIA_filtered, f)        f.close()        print('Done!\n')    else:        print('MMIA data was pre-conditioned. Uploading from %s/mmia_filtered_data.pckl...' % ssd_path)        f = open(MMIA_filtered_bin + '/' + matches[day] + '.pckl', 'rb')        MMIA_filtered = pickle.load(f)        f.close()        print('Done!\n')    del mmia_raw    ########### GLM'S DATA DOWNLOAD, EXTRACTION, UPLOAD AND CONDITIONING ###########    # Downloading GLM data from Google Cloud Services    if pre_downloaded_GLM == False:                TFG.download_GLM(ssd_path, trigger_filenames, MMIA_filtered, matches)    del trigger_filenames    # Extracting GLM data into trigger .txt files    if pre_extracted_GLM == False:        TFG.extract_GLM(GLM_ordered_dir, GLM_ordered_outputs, trigger_limits, matches, MMIA_filtered, angle_margin, cropping_margin)    del trigger_limits    # Uploading and integrating GLM signal    if pre_integrated_GLM == False:                # Unifying all data in a structure of matrices        GLM_raw_data = TFG.unify_GLM_data(GLM_ordered_outputs, MMIA_filtered, matches, show_plots)        # Conditioning GLM data for further analysis        GLM_data = TFG.condition_GLM_data(GLM_raw_data, matches, show_plots)        del GLM_raw_data                print('Saving GLM integrated data (this process can take some seconds)...')        f = open('glm_integrated_data.pckl', 'wb')        pickle.dump(GLM_data, f)        f.close()        print('Done!\n')    else:        print('GLM data was pre-integrated. Uploading from %s/glm_integrated_data.pckl...' % ssd_path)        f = open('glm_integrated_data.pckl', 'rb')        GLM_data = pickle.load(f)        f.close()        print('Done!\n')    ########### CROSS-CORRELATION ###########    if pre_xc == False:                # Normalizing GLM data to cross-correlate with MMIA data        print('Normalizing GLM data...')        GLM_norm = [None] * len(GLM_data)        for i in range(len(GLM_data)):            snip = [None] * len(GLM_data[i])            GLM_norm[i] = snip        for i in range(len(GLM_data)):            for j in range(len(GLM_data[i])):                if type(GLM_data[i][j]) == np.ndarray:                    snippet = np.zeros((len(GLM_data[i][j]),2))                    GLM_norm[i][j] = snippet                    GLM_norm[i][j][:,0] = GLM_data[i][j][:,0]                    GLM_norm[i][j][:,1] = TFG.normalize(GLM_data[i][j][:,1])        print('Done!')        print(' ')        # Normalizing MMIA data to cross-correlate with GLM data        print('Normalizing MMIA data...')        MMIA_norm = [None] * len(MMIA_filtered)        for i in range(len(MMIA_filtered)):            snip = [None] * len(MMIA_filtered[i])            MMIA_norm[i] = snip        for i in range(len(MMIA_filtered)):            for j in range(len(MMIA_filtered[i])):                if type(MMIA_filtered[i][j]) == np.ndarray:                    snippet = np.zeros((len(MMIA_filtered[i][j]),2))                    MMIA_norm[i][j] = snippet                    MMIA_norm[i][j][:,0] = MMIA_filtered[i][j][:,0]                    MMIA_norm[i][j][:,1] = TFG.normalize(MMIA_filtered[i][j][:,1])        print('Done!')        print(' ')        # Cross-correlating snippets        [GLM_xcorr, MMIA_xcorr, GLM_xcorr_norm, MMIA_xcorr_norm, delays] = TFG.cross_correlate_GLM_MMIA(GLM_data, MMIA_filtered, GLM_norm, MMIA_norm, matches, show_plots)        del GLM_norm        del MMIA_norm        # Saving cross-correlated data        print('Saving all cross-correlated data (this process can take some seconds)...')        f = open('GLM_MMIA_xcorr_data.pckl', 'wb')        pickle.dump([matches, GLM_xcorr, MMIA_xcorr, GLM_xcorr_norm, MMIA_xcorr_norm], f)        f.close()        print('Done!\n')    else:        print('GLM and MMIA data was pre-correlated. Uploading from %s/GLM_MMIA_xcorr_data.pckl...' % ssd_path)        f = open('GLM_MMIA_xcorr_data.pckl', 'rb')        [matches, GLM_xcorr, MMIA_xcorr, GLM_xcorr_norm, MMIA_xcorr_norm] = pickle.load(f)        f.close()        print('Done!\n')    del GLM_data    del MMIA_filtered    ########### PEAK DETECTION AND COMPARISON ###########    # Getting peaks from cross-correlated signals    [GLM_peaks, MMIA_peaks] = TFG.get_GLM_MMIA_peaks(GLM_xcorr, MMIA_xcorr, matches, show_plots)    f = open('peaks_data.pckl', 'wb')    pickle.dump([GLM_peaks, MMIA_peaks], f)    f.close()    # Getting matching peaks    matching_peaks = TFG.get_peak_matches(GLM_xcorr, MMIA_xcorr, GLM_peaks, MMIA_peaks, show_plots, matches)    '''    # Getting delay statistics    [total_snippets, avg_all, std_all, avg_MMIA_delay, std_MMIA_delay, avg_GLM_delay, std_GLM_delay, MMIA_delays, GLM_delays, no_delays] = TFG.study_delays(delays, GLM_xcorr, MMIA_xcorr, show_plots)    '''